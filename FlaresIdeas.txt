Try mlframe:
    V Build separate full labelled featureset
    V Make generous train/val/test split
    V Run mlframe
    V lstms, rnns, grus classifiers to be fed with entire light curve? [WORKS! beats static features in ROC & PR AUCs!!!]
    
Finding separate most discriminative features:

    compute featuresets: big, oos

    V then for every feature f, and decision_function in (mean, median, std), compute ratios decision_function(f(big))/decision_function(f(big)), sort features by ratios.

Features:
    Для всех рядов, на этапе препроцессинга выкидывать/заменять средним i+1,i единичные СЛИШКОМ УЖ аномальные выбросы (только если их количество строго равно 1)?    
    more features like norm_max_consecutive_below_2sigma
    velocity 1,2 stats
    stats over series up to/after argmax/argmin, separately. also between argmin & argmax.
    скорости и norm (да и все статпризнаки) считать не только от сырого, а и от сглаженного ряда (sma, ema, kalman)
    more stats over wavelets?
    stats from polars-ds, functime?


Created [sample_probability_plotter.py](Machine Learning/Astronomy/sample_probability_plotter.py). Usage:

python sample_probability_plotter.py \
    --parquet_path path/to/iter0200_all_probabilities.parquet \
    --dataset path/to/hf_dataset \
    --min_prob 0.3 \
    --max_prob 0.7 \
    --num_samples 50 \
    --output_dir sampled_plots/
Parameters:
--parquet_path - Path to probability parquet file (has row_index, flare_prob, optional id)
--dataset - Path to the HuggingFace dataset directory
--min_prob / --max_prob - Probability range filter (0-1)
--num_samples - Number of random samples to plot
--output_dir - Where to save plots
Optional:
--seed - Random seed for reproducibility
--outlier_factor - Singlepoint outlier factor for view_series (default: 10.0)
--show - Display plots interactively
Output files are named: sample_{id}_row{index}_P{prob}pct.png


    
ACTIVE LEARNING:

V Need to change the labelling approach. Instead of labelling top-N most certain predictions (highest prob), label top-N most uncertain ones each time (highest abs(prob-0.5))


out of limited # of known flares, create much wider simulated set when only parts of the series are left, or random parts are missing (simulate non-observation periods)

Для seed samples, создавать много синтетики:
    1) удаляя произвольную точку
    2) добавляя ряд до и после вспышки ?


V Report FI on each iter
lgb,xgb, cb as bootstrap models?

2-phase training?
    1) whole dataset
    2) top-200k by P(flare) only? or maybe clustering?
    
New crowd-labelling platform?
    Social login
Responsible:
    Inviting to projects
    Ratings
Expert quality estimation:
    Concordance scores
    Detailed logging
Multiple featuresets versions (for the same project/target)
Parallel cloud models
Live FIs (Shap)
When some one opens a taks for a first time (or without logging into it for last 2 weeks, or when new instrcutions arrived),
    he is presented with common rules & examples & edge cases of how to classify properly.
    Also labelelr can be TOOLED with specific external links, frames etc (incl keyed).


Стратегия Экспертной Разметки
Вопрос: Как выбрать ~20 примеров для эксперта?
Оптимальная стратегия: Stratified Uncertainty Sampling

def select_for_expert_labeling(
    features: np.ndarray,
    probas: np.ndarray,
    n_samples: int = 20,
) -> list[int]:
    """
    Выбираем примеры для эксперта так, чтобы:
    1. Покрыть разные регионы feature space
    2. Включить примеры с разной уверенностью модели
    3. Максимизировать информативность
    """
    from sklearn.cluster import KMeans

    # 1. Cluster ALL data (not just uncertain)
    n_clusters = n_samples // 2  # ~10 clusters
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(features)

    selected = []

    for cluster_id in range(n_clusters):
        cluster_mask = cluster_labels == cluster_id
        cluster_indices = np.where(cluster_mask)[0]
        cluster_probas = probas[cluster_indices]

        # Из каждого кластера берём:
        # 1. Один НАИБОЛЕЕ uncertain (P ≈ 0.5)
        uncertainty = np.abs(cluster_probas - 0.5)
        most_uncertain = cluster_indices[np.argmin(uncertainty)]
        selected.append(most_uncertain)

        # 2. Один НАИБОЛЕЕ confident positive (P > 0.9)
        high_conf_mask = cluster_probas > 0.9
        if np.any(high_conf_mask):
            high_conf_indices = cluster_indices[high_conf_mask]
            selected.append(high_conf_indices[0])

    return selected[:n_samples]
Почему лучше чем просто UMAP центры:
UMAP/t-SNE теряют информацию → кластеры могут быть артефактами
Мы учитываем И feature space, И model uncertainty
Получаем примеры для проверки И decision boundary, И high-confidence predictions